{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba9f8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import random\n",
    "import pathlib\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PraNet.utils.dataloader import test_dataset\n",
    "from PraNet.lib.PraNet_Res2Net import PraNet\n",
    "from skimage.filters import gaussian\n",
    "from skimage.morphology import label as connected_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c45167",
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf8bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_randomness(seed=0):\n",
    "    np.random.seed(seed=seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d6bb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Code for the local maximum version of the method.\n",
    "\"\"\"\n",
    "\n",
    "from skimage.morphology import reconstruction\n",
    "from skimage.measure import regionprops\n",
    "def find_peaks(sigmoid):\n",
    "    sigmoid = gaussian(sigmoid,0.5)\n",
    "    seed = np.copy(sigmoid)\n",
    "    seed[1:-1, 1:-1] = sigmoid.min()\n",
    "    mask = sigmoid\n",
    "    dilated = reconstruction(seed, mask, method='dilation')\n",
    "    peaks = (sigmoid - dilated)\n",
    "    binarized_peaks = peaks > 0.05\n",
    "    labels, num_components = connected_components(binarized_peaks, background=0, return_num=True, connectivity=2)\n",
    "    proposals = regionprops(labels, intensity_image=None, cache=True)\n",
    "    normalization_value = np.ones_like(peaks)\n",
    "    minsize = 25 \n",
    "    for region in proposals:\n",
    "        # take regions with large enough areas\n",
    "        if region.area >= minsize:\n",
    "        # draw rectangle around segmented coins\n",
    "            minr, minc, maxr, maxc = region.bbox\n",
    "            minr = max(minr-minsize, 0)\n",
    "            minc = max(minc-minsize, 0)\n",
    "            maxr = min(maxr+minsize, normalization_value.shape[0]-1)\n",
    "            maxc = min(maxc+minsize, normalization_value.shape[1]-1)\n",
    "            np.minimum(normalization_value[minr:maxr, minc:maxc], peaks[minr:maxr, minc:maxc].max(), out = normalization_value[minr:maxr, minc:maxc])\n",
    "    peaks = np.maximum(sigmoid, peaks/normalization_value)\n",
    "\n",
    "    return peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30e3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_examples(folders):\n",
    "    num = 0\n",
    "    for folder in folders:\n",
    "        num += len([name for name in os.listdir(folder + '/images/')])\n",
    "    return num\n",
    "\n",
    "def get_data(cache_path):\n",
    "    #base model path\n",
    "    model_path = './PraNet/snapshots/PraNet_Res2Net/PraNet-19.pth'\n",
    "    #image size\n",
    "    test_size = 352\n",
    "    T = 10 \n",
    "    folders = ['HyperKvasir', 'CVC-300', 'CVC-ClinicDB', 'Kvasir', 'CVC-ColonDB', 'ETIS-LaribPolypDB']\n",
    "    folders = ['./PraNet/data/TestDataset/' + x for x in folders]\n",
    "\n",
    "    try:\n",
    "        img_names = np.load(cache_path + 'img_names.npy')\n",
    "        sigmoids = np.load(cache_path + 'sigmoids.npy')\n",
    "        masks = np.load(cache_path + 'masks.npy')\n",
    "        regions = np.load(cache_path + 'regions.npy')\n",
    "        num_components = np.load(cache_path + 'num_components.npy')\n",
    "        print(f'Loaded {sigmoids.shape[0]} labeled examples from cache.')\n",
    "    except:\n",
    "        #calculate number of images\n",
    "        num_examples = get_num_examples(folders)\n",
    "        print(f'Caching {num_examples} labeled examples.')\n",
    "        img_names = ['']*num_examples\n",
    "        sigmoids = np.zeros((num_examples,test_size, test_size))\n",
    "        masks = np.zeros((num_examples,   test_size, test_size))\n",
    "        regions = np.zeros((num_examples, test_size, test_size))\n",
    "        num_components = np.zeros((num_examples,))\n",
    "        \n",
    "        k = 0\n",
    "\n",
    "        for data_path in folders:\n",
    "            model = PraNet()\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model.cuda()\n",
    "            model.eval()\n",
    "\n",
    "            os.makedirs(cache_path, exist_ok=True)\n",
    "            image_root = '{}/images/'.format(data_path)\n",
    "            gt_root = '{}/masks/'.format(data_path)\n",
    "            test_loader = test_dataset(image_root, gt_root, test_size)\n",
    "\n",
    "            for i in range(test_loader.size):\n",
    "                image, gt, name = test_loader.load_data()\n",
    "                print(f\"\\33[2K\\r Processing {name}\", end=\"\")\n",
    "                gt = np.asarray(gt, np.float32)\n",
    "                gt /= (gt.max() + 1e-8)\n",
    "                image = image.cuda()\n",
    "\n",
    "                res5, res4, res3, res2 = model(image)\n",
    "                \n",
    "                # Populate the arrays\n",
    "                img_names[k] = image_root + '/' + name\n",
    "                sigmoids[k,:,:] = (res2/T).sigmoid().detach().cpu().numpy()\n",
    "                temp_mask = resize(gt, (test_size, test_size), anti_aliasing=False)\n",
    "                #temp_mask = area_opening(temp_mask, area_threshold=10, connectivity=2)\n",
    "                #temp_mask = area_closing(temp_mask, area_threshold=10, connectivity=2)\n",
    "                temp_mask = gaussian(temp_mask, sigma=1, multichannel=True)\n",
    "                temp_mask[temp_mask > 0.5] = 1\n",
    "                masks[k,:,:], num_components[k] = connected_components(temp_mask, background=0, return_num=True, connectivity=2)\n",
    "                regions[k,:,:] = find_peaks(sigmoids[k,:,:])\n",
    "                k += 1\n",
    "        os.makedirs(cache_path, exist_ok=True)\n",
    "        np.save(cache_path + 'sigmoids', sigmoids)\n",
    "        np.save(cache_path + 'img_names', img_names)\n",
    "        np.save(cache_path + 'masks', masks)\n",
    "        np.save(cache_path + 'regions', regions)\n",
    "        np.save(cache_path + 'num_components', num_components)\n",
    "    return np.array(img_names), torch.tensor(sigmoids), torch.tensor(masks),torch.tensor(regions), torch.tensor(num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce85318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_loss_and_size_tables(regions, masks, lambdas_example_table, num_calib):\n",
    "    lam_len = len(lambdas_example_table)\n",
    "    lam_low = min(lambdas_example_table)\n",
    "    lam_high = max(lambdas_example_table)\n",
    "    fname_loss = f'.cache/{lam_low}_{lam_high}_{lam_len}_example_loss_table.npy'\n",
    "    fname_sizes = f'.cache/{lam_low}_{lam_high}_{lam_len}_example_size_table.npy'\n",
    "    try:\n",
    "        loss_table = np.load(fname_loss)\n",
    "        sizes_table = np.load(fname_sizes)\n",
    "    except:\n",
    "        print(\"computing loss and size table\")\n",
    "        loss_table = np.zeros((regions.shape[0], lam_len))\n",
    "        sizes_table = np.zeros((regions.shape[0], lam_len))\n",
    "        for j in tqdm(range(lam_len)):\n",
    "            est_regions = (regions >= -lambdas_example_table[j])\n",
    "            print(type(est_regions),est_regions.shape,masks.shape)\n",
    "            loss_table[:,j] = loss_perpolyp_01(est_regions, regions, masks) \n",
    "            sizes_table[:,j] = est_regions.sum(dim=1).sum(dim=1)/masks.sum(dim=1).sum(dim=1)\n",
    "\n",
    "        np.save(fname_loss, loss_table)\n",
    "        np.save(fname_sizes, sizes_table)\n",
    "\n",
    "    return loss_table, sizes_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca853a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_perpolyp_01(T, risk_mass, masks): # lambda in [-1,0]\n",
    "    # Get the missed pixels \n",
    "    num_polyps = masks.max(dim=1)[0].max(dim=1)[0]\n",
    "    missed = ((masks>0).to(int) - T.to(int)) # as lambda grows, the sets grow.\n",
    "    F.relu(missed, inplace=True) \n",
    "    # Split the different polyps into different rows.\n",
    "    missed = missed * masks\n",
    "    results_perpolyp = torch.zeros((num_polyps.sum().int().item(), masks.shape[1], masks.shape[2]))\n",
    "    masks_perpolyp = torch.zeros_like(results_perpolyp)\n",
    "    k = 0\n",
    "    for n in range(num_polyps.max().int().item()):\n",
    "        filter_bool = (num_polyps >= n + 1) # 1, 2, 3 polyps \n",
    "        temp_missed = missed[filter_bool]\n",
    "        temp_masks = masks[filter_bool]\n",
    "        results_perpolyp[k:k+temp_missed.shape[0]] = (temp_missed == n + 1)\n",
    "        masks_perpolyp[k:k+temp_missed.shape[0]] = (temp_masks == n+1)\n",
    "        k += temp_missed.shape[0] \n",
    "    results_perpolyp = results_perpolyp.to(float).sum(dim=1).sum(dim=1)/masks_perpolyp.sum(dim=1).sum(dim=1)    \n",
    "    return results_perpolyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e52527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_precomputed(example_loss_table, example_size_table, alpha, num_calib, num_lam, lambdas_example_table):\n",
    "    total=example_loss_table.shape[0]\n",
    "    perm = torch.randperm(example_loss_table.shape[0])\n",
    "    example_loss_table = example_loss_table[perm]\n",
    "    example_size_table = example_size_table[perm]\n",
    "    calib_losses, val_losses = (example_loss_table[0:num_calib], example_loss_table[num_calib:])\n",
    "    calib_sizes, val_sizes = (example_size_table[0:num_calib], example_size_table[num_calib:])\n",
    "\n",
    "    lhat = get_lhat(calib_losses[:,::-1], lambdas_example_table[::-1], alpha)\n",
    "\n",
    "    losses = val_losses[:,np.argmax(lambdas_example_table == lhat)]\n",
    "    #print(lhat,losses)\n",
    "    size = np.random.choice(val_sizes[:,np.argmax(lambdas_example_table == lhat)])\n",
    "\n",
    "    return lhat, losses.mean(), size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7081a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lhat(calib_loss_table, lambdas, alpha, B=1):\n",
    "    n = calib_loss_table.shape[0]\n",
    "    rhat = calib_loss_table.mean(axis=0)\n",
    "#     print()\n",
    "    lhat_idx = max(np.argmax(((n/(n+1)) * rhat + B/(n+1)) >= alpha) - 1, 0) # Can't be -1.\n",
    "    print(lambdas[lhat_idx])\n",
    "    return lambdas[lhat_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9fda292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(alpha, num_trials, num_calib, num_lam, output_dir, lambdas_example_table):\n",
    "    img_names, sigmoids, masks, regions, num_components = get_data(cache_path)\n",
    "    masks[masks > 1] = 1\n",
    "    fname = cache_path + f'{alpha}_{num_calib}_{num_lam}_dataframe'.replace('.','_') + '.pkl'\n",
    "\n",
    "    df = pd.DataFrame(columns=['$\\\\hat{\\\\lambda}$','risk','sizes','alpha'])\n",
    "    try:\n",
    "        print('Dataframe loaded')\n",
    "        df = pd.read_pickle(fname)\n",
    "    except:\n",
    "        example_loss_table, example_sizes_table = get_example_loss_and_size_tables(regions, masks, \\\n",
    "                                                                                   lambdas_example_table, \\\n",
    "                                                                                   num_calib)\n",
    "\n",
    "        local_df_list = []\n",
    "        for i in tqdm(range(num_trials)):\n",
    "            lhat, risk, sizes = trial_precomputed(example_loss_table, example_sizes_table, alpha, num_calib, num_lam, lambdas_example_table)\n",
    "                \n",
    "            dict_local = {\"$\\\\hat{\\\\lambda}$\": lhat, \"risk\": risk, \"sizes\": sizes, \"alpha\": alpha}\n",
    "            df_local = pd.DataFrame(dict_local, index=[i])\n",
    "            local_df_list = local_df_list + [df_local]\n",
    "        df = pd.concat(local_df_list, axis=0, ignore_index=True)\n",
    "        df.to_pickle(fname)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cadb4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(df, alpha, output_dir):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12,3))\n",
    "    axs[0].hist(df['risk'].to_numpy(), alpha=0.7, density=True)\n",
    "\n",
    "    normalized_size = df['sizes'].to_numpy()\n",
    "    axs[1].hist(normalized_size, bins=60, alpha=0.7, density=True)\n",
    "\n",
    "    axs[0].set_xlabel('risk')\n",
    "    axs[0].locator_params(axis='x', nbins=10)\n",
    "    axs[0].axvline(x=alpha,c='#999999',linestyle='--',alpha=0.7)\n",
    "    axs[0].set_ylabel('density')\n",
    "    axs[1].set_xlabel('set size as a fraction of polyp size')\n",
    "    axs[1].locator_params(axis='x', nbins=10)\n",
    "    axs[1].set_yscale('log')\n",
    "    #axs[1].legend()\n",
    "    sns.despine(top=True, right=True, ax=axs[0])\n",
    "    sns.despine(top=True, right=True, ax=axs[1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig( output_dir + (f'{alpha}_polyp_histograms').replace('.','_') + '.pdf'  )\n",
    "    print(f\"The mean and standard deviation of the risk over {len(df)} trials are {df['risk'].mean()} and {df['risk'].std()} respectively.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36873432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching 898 labeled examples.\n",
      " Processing cju0u82z3cuma0835wlxrnrjv.png"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16174/775773311.py:61: FutureWarning: `multichannel` is a deprecated argument name for `gaussian`. It will be removed in version 1.0.Please use `channel_axis` instead.\n",
      "  temp_mask = gaussian(temp_mask, sigma=1, multichannel=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing 99.png\u001b[2KK3i0818ev74qpxq.png\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2K\u001b[2KDataframe loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/1000 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.004008016032064243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        sns.set(palette='pastel', font='serif')\n",
    "        sns.set_style('white')\n",
    "        fix_randomness()\n",
    "\n",
    "        cache_path = './.cache/'\n",
    "        output_dir = 'outputs/histograms/'\n",
    "        pathlib.Path(cache_path).mkdir(parents=True, exist_ok=True)\n",
    "        pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        num_trials = 1000\n",
    "        num_calib = 700\n",
    "        num_lam = 500\n",
    "        alpha = 0.1\n",
    "        lambdas_example_table = np.linspace(-1,0,500)\n",
    "\n",
    "        df = experiment(alpha, num_trials, num_calib, num_lam, output_dir, lambdas_example_table)\n",
    "        plot_histograms(df, alpha, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
